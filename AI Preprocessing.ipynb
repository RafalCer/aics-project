{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The person has big lips, wavy hair, arched eyebrows, high cheekbones, bags under eyes, brown hair, mouth slightly open, and pointy nose. She is wearing heavy makeup.'\n",
      "'This person has arched eyebrows, wavy hair, and mouth slightly open. She wears lipstick. She is attractive.'\n",
      "'This man is chubby and has oval face, big nose, high cheekbones, arched eyebrows, goatee, receding hairline, and big lips.'\n",
      "'This person is attractive and has wavy hair, bags under eyes, mouth slightly open, and pale skin.'\n",
      "'This man has double chin, bags under eyes, big lips, narrow eyes, big nose, and mouth slightly open. He is young. He wears hat. He has beard.'\n",
      "'This man has big lips, big nose, double chin, bangs, high cheekbones, bags under eyes, and eyeglasses. He is smiling. He has no beard.'\n",
      "'This person has bags under eyes. He is young. He has beard.'\n",
      "'This person has pointy nose, and mouth slightly open and wears heavy makeup, and lipstick. She is young, and smiling.'\n",
      "'This woman has high cheekbones, arched eyebrows, and black hair. She wears lipstick. She is attractive.'\n",
      "'This person has pale skin, and straight hair. He is attractive.'\n",
      "'This woman has oval face, pointy nose, rosy cheeks, arched eyebrows, big lips, wavy hair, and blond hair. She is young, and smiling. She is wearing heavy makeup.'\n",
      "'The woman has oval face, bangs, and wavy hair. She is wearing heavy makeup. She is attractive.'\n",
      "'The person has bags under eyes, high cheekbones, big nose, and straight hair and wears necktie.'\n",
      "'This person has high cheekbones, black hair, rosy cheeks, wavy hair, pointy nose, and arched eyebrows. She is smiling, and young. She is wearing lipstick.'\n",
      "'The woman wears heavy makeup. She has wavy hair, and black hair. She is attractive.'\n",
      "'She is wearing necklace, and heavy makeup. She has arched eyebrows, bangs, blond hair, and high cheekbones. She is smiling, and young.'\n",
      "'This person has straight hair, and brown hair.'\n",
      "'This person has bags under eyes, big nose, and receding hairline.'\n",
      "'The woman has mouth slightly open, and black hair. She is young. She is wearing lipstick.'\n",
      "'The woman has arched eyebrows, straight hair, high cheekbones, oval face, pointy nose, and rosy cheeks. She is young, and smiling and is wearing heavy makeup.'\n",
      "'The person has black hair, pointy nose, arched eyebrows, and straight hair and wears heavy makeup. She is attractive.'\n"
     ]
    }
   ],
   "source": [
    "# Extracting the longest captions for all images\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def parse_captions(images_dir=None, nr_images):\n",
    "    df = pd.DataFrame(columns=['Img_name', 'Caption', 'Img_path'])\n",
    "    \n",
    "    if images_dir:\n",
    "        os.chdir(images_dir)\n",
    "    else:\n",
    "        os.chdir(r'C:\\Users\\Rafa\\Downloads\\text\\celeba-caption')\n",
    "\n",
    "    files = os.listdir()\n",
    "\n",
    "    for i, file in enumerate(files):\n",
    "        if i > nr_images:\n",
    "            return df\n",
    "        else:\n",
    "            with open(file, 'r') as infile:\n",
    "                descriptions = infile.readlines()\n",
    "\n",
    "                longest = descriptions[0].strip()\n",
    "                for description in descriptions:\n",
    "                    if len(description.strip()) > len(longest):\n",
    "                        longest = description.strip().replace(r'\\n', '')\n",
    "                \n",
    "                print(repr(longest))\n",
    "\n",
    "                img_name = file.replace('txt', 'jpg')\n",
    "                img_path = '/home/guscerra@GU.GU.SE/aics-project/celeb_images/CelebA-HQ-img/' + img_name\n",
    "                df_row = {'Img_name': img_name, 'Caption':longest, 'Img_path':img_path}\n",
    "                df = df.append(df_row, ignore_index=True)\n",
    "\n",
    "\n",
    "    return df\n",
    "        \n",
    "df = parse_captions(nr_images=20)\n",
    "# df.to_csv(r'C:\\Users\\Rafa\\Downloads\\text\\celeb_preprocessed.csv', sep='\\t')\n",
    "df.to_csv(r'C:\\Users\\Rafa\\Downloads\\text\\celeb_preprocessed_test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the sketches\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "directory = r\"C:\\Users\\Rafa\\Downloads\\sketchai\\imgs\"\n",
    "os.chdir(r'C:\\Users\\Rafa\\Downloads\\sketchai\\imgs')\n",
    "\n",
    "# sketches\n",
    "for i in range(10000):\n",
    "    # sketches\n",
    "\n",
    "#     img_name = \"fakeA_0_\" + str(i) + \".jpg\"\n",
    "#     img_w_path = directory + \"\\\\\" + img_name\n",
    "#     new_name = str(i) + \".jpg\"\n",
    "#     os.rename(img_name, new_name)\n",
    "\n",
    "    # pics\n",
    "    \n",
    "    img_name = \"inputA_0_\" + str(i) + \".jpg\"\n",
    "    img_w_path = directory + \"\\\\\" + img_name\n",
    "    new_name = r'C:\\Users\\Rafa\\Downloads\\sketchai\\pics' + '\\\\' + str(i) + \".jpg\"\n",
    "    shutil.move(img_w_path, new_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating\n",
      "2 0\n",
      "['person', 'pale', 'skin', 'big', 'nose']\n",
      "['young', 'woman', 'pointy', 'nose', 'big', 'lips']\n"
     ]
    }
   ],
   "source": [
    "# Checking whether the generated sketches match the original ones in the data set \n",
    "\n",
    "import os\n",
    "import json \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "captions_dir = r\"C:\\Users\\Rafa\\Downloads\\text\\celeba-caption\"\n",
    "\n",
    "predictions_file = open(r\"C:\\Users\\Rafa\\Downloads\\predictions_on_skt_m3_img.json\", 'rb')\n",
    "predictions_dict = json.load(predictions_file)\n",
    "\n",
    "def fiter_out_stop_words(sentence):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tokenized_sent = [word.lower().strip() for word in tokenized_sent if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('English'))\n",
    "    filtered_caption = [word for word in tokenized_sent if word not in stop_words]\n",
    "    return filtered_caption\n",
    "                     \n",
    "''' Without filtering out the stop words '''\n",
    "# for i in predictions_dict.keys():\n",
    "#     # dropping <start> and <end> tags and turning the caption list into a string\n",
    "#     predicted_caption = (' ').join(predictions_dict[i][1:-1])\n",
    "    \n",
    "#     # loading the original captions\n",
    "#     filename = captions_dir + '\\\\' + i + '.txt'\n",
    "#     with open(filename, 'r', encoding='utf-8') as f:\n",
    "#         real_captions = f.readlines()\n",
    "#     # matching the predicted captions\n",
    "#     real_captions = [caption.lower().strip().replace('.', '').replace(',','') for caption in real_captions]\n",
    "    \n",
    "#     if predicted_caption in real_captions:\n",
    "#         print(i)\n",
    "\n",
    "''' Stop words filtered out '''\n",
    "for i in predictions_dict.keys():\n",
    "    # dropping <start> and <end> tags and turning the caption list into a string\n",
    "    predicted_caption = (' ').join(predictions_dict[i][1:-1])\n",
    "    \n",
    "    # loading the original captions\n",
    "    filename = captions_dir + '\\\\' + i + '.txt'\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        real_captions = f.readlines()\n",
    "\n",
    "    # pre-processing the predicted captions\n",
    "    real_captions = [caption.lower().strip().replace('.', '').replace(',','') for caption in real_captions]\n",
    "    \n",
    "    # dropping the stop words from PREDICTED\n",
    "    predicted_caption = fiter_out_stop_words(predicted_caption)\n",
    "\n",
    "    # dropping the stop words from PREDICTED\n",
    "    real_captions = [fiter_out_stop_words(caption) for caption in real_captions]\n",
    "\n",
    "    \n",
    "    if predicted_caption in real_captions:\n",
    "        print(i)\n",
    "    \n",
    "    max_matching_words = 0\n",
    "    best_match = None\n",
    "    \n",
    "    for i, r_c in enumerate(real_captions):\n",
    "        matching_words = 0\n",
    "        for word in r_c:\n",
    "            if word in predicted_caption:\n",
    "                matching_words += 1\n",
    "                \n",
    "        if matching_words > max_matching_words:\n",
    "            print('updating')\n",
    "            max_matching_words = matching_words\n",
    "            best_match = i\n",
    "            \n",
    "    print(max_matching_words, best_match)\n",
    "    \n",
    "    print(predicted_caption)\n",
    "    print(real_captions[best_match])\n",
    "    \n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
