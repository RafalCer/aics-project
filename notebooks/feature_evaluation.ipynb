{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUMENTS\n",
    "\n",
    "# main_path = 'C:\\Users\\Rafa\\Downloads\\'\n",
    "# path_to_images = '/home/guscerra@GU.GU.SE/aics-project/celeb_images/CelebA-HQ-img/'\n",
    "# captions_dir = r\"C:\\Users\\Rafa\\Downloads\\text\\celeba-caption\"\n",
    "\n",
    "dataset = 'celeb_small_res'\n",
    "main_path = '/Users/evelsve/repos/cap'\n",
    "path_to_images = main_path + f'/data/{dataset}'\n",
    "captions_dir = f'/Users/evelsve/repos/cap/data/captions'\n",
    "\n",
    "\n",
    "predictions_file = \"predictions_m5_sketches.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the generated sketches match the original ones in the data set \n",
    "\n",
    "import os\n",
    "import json \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def fiter_out_stop_words(sentence):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tokenized_sent = [word.lower().strip() for word in tokenized_sent if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('English'))\n",
    "    filtered_caption = [word for word in tokenized_sent if word not in stop_words]\n",
    "    return filtered_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caps(predictions_dict, captions_dir):\n",
    "\n",
    "    predicted_captions = list()\n",
    "    real_captions = list()\n",
    "\n",
    "    #''' Stop words filtered out '''\n",
    "    for i in predictions_dict.keys():\n",
    "        # dropping <start> and <end> tags and turning the caption list into a string\n",
    "        predicted_caption = (' ').join(predictions_dict[i][1:-1])\n",
    "\n",
    "        # dropping the stop words from PREDICTED\n",
    "        predicted_caption = fiter_out_stop_words(predicted_caption)\n",
    "        \n",
    "        predicted_captions.append(predicted_caption)\n",
    "\n",
    "        # loading the original captions\n",
    "        # real_captions_path = captions_dir + '\\\\' + i + '.txt'\n",
    "        real_caption_path = captions_dir + '/' + i + '.txt'\n",
    "        with open(real_caption_path, 'r', encoding='utf-8') as f:\n",
    "            real_caption = f.readlines()\n",
    "\n",
    "        # pre-processing the predicted captions\n",
    "        real_caption = [caption.lower().strip().replace('.', '').replace(',','') for caption in real_caption]\n",
    "        # dropping the stop words from PREDICTED\n",
    "        real_caption = [fiter_out_stop_words(caption) for caption in real_caption]\n",
    "        real_captions.append(real_caption)\n",
    "        \n",
    "    return predicted_captions, real_captions\n",
    "    \n",
    "    \n",
    "def reformat_to_bigrams(data):\n",
    "    reformatted_to_bigrams = list()\n",
    "    for sentence in data:\n",
    "        bigrammed = list()\n",
    "        for i, word in enumerate(sentence[:-1]):\n",
    "            bigram = (sentence[i], sentence[i+1])\n",
    "            bigrammed.append(bigram)\n",
    "        reformatted_to_bigrams.append(bigrammed)\n",
    "    return reformatted_to_bigrams\n",
    "\n",
    "\n",
    "def evaluate(predicted, actual, f):\n",
    "    predicted = reformat_to_bigrams(predicted)\n",
    "    \n",
    "    new_actual = list()\n",
    "    for item in actual:\n",
    "        item = reformat_to_bigrams(item)\n",
    "        new_actual.append(item)\n",
    "    \n",
    "    actual = new_actual\n",
    "    \n",
    "    scores = dict()\n",
    "\n",
    "    for i1, predicted_caption in enumerate(predicted):\n",
    "        \n",
    "        scores[i1] = dict()\n",
    "        \n",
    "        for i2, real_caption in enumerate(actual[i1]):\n",
    "            if predicted_caption == real_caption:\n",
    "                f.write(f'\\nComplete match at image {i1} caption number {i2}')\n",
    "            tp = 0\n",
    "            for tupler in real_caption:\n",
    "                if tupler in predicted_caption:\n",
    "                    tp += 1\n",
    "            try:\n",
    "                score = tp/len(real_caption)*100\n",
    "            except:\n",
    "                score = 0\n",
    "\n",
    "            scores[i1][i2] = score\n",
    "            \n",
    "    return scores\n",
    "\n",
    "\n",
    "def average_evaluations(scores, actual, maximize=True):\n",
    "    new_scores = list()\n",
    "    for img_id, per_caption_scores in scores.items():\n",
    "        per_caption_scores = per_caption_scores.values()\n",
    "        score_list = list(per_caption_scores)\n",
    "        if maximize:\n",
    "            biggest = max(score_list)\n",
    "            new_scores.append(biggest)\n",
    "        else:\n",
    "            avg = sum(score_list)/10\n",
    "            new_scores.append(avg)\n",
    "    return sum(new_scores)/len(actual)\n",
    "\n",
    "\n",
    "def pipeline(main_path):\n",
    "    predictions_dir = main_path + '/predictions'\n",
    "    with open(f'{main_path}/feature_eval_results.txt', mode='w') as f:\n",
    "        for predictions_file in listdir(main_path + '/predictions'):\n",
    "            if predictions_file[-5:] == '.json' and predictions_file != 'augmented.json':\n",
    "                f.write(f'\\n--- {predictions_file} ---\\n')\n",
    "                predictions_path = open(predictions_dir + '/' + predictions_file, 'rb')\n",
    "                predictions_dict = json.load(predictions_path)\n",
    "                predicted, actual = get_caps(predictions_dict, captions_dir)\n",
    "                scores = evaluate(predicted, actual, f)\n",
    "                f.write(f'\\nResult: {average_evaluations(scores, actual)}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
