{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e0d39c",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a016e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T19:03:42.566307Z",
     "start_time": "2022-01-17T19:03:42.546720Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from collections import Counter\n",
    "from random import seed, choice, sample\n",
    "\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8d0e9",
   "metadata": {},
   "source": [
    "In the cell below one chooses th arguments for the preprocessing of data before training the model. The `dataset` is a folder with `captions` and `img` directories, where one could find images in `.jpg` format and captions for every image in `.txt` format. The images and captions are linked through `image_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cap_lenght = 60\n",
    "cap_per_img = 5\n",
    "min_word_freq = 10\n",
    "\n",
    "#PAY ATTENTION HERE WHEN RERUNNING\n",
    "# dataset = 'dataset_to_train_on'\n",
    "# folder = 'where_output_will_go'\n",
    "\n",
    "dataset = 'celeb_small_res'\n",
    "folder = 'trial'\n",
    "\n",
    "#Choose only one or none as TRUE\n",
    "full_anton_captions = False\n",
    "partial_anton_captions = False\n",
    "mix_data = True\n",
    "mix_captions = False\n",
    "#-----------------\n",
    "\n",
    "base_filename = f'{dataset}_{cap_per_img}_{min_word_freq}'\n",
    "main_path = '/Users/evelsve/repos/cap'\n",
    "out_path = main_path+ f'/preprocessing_outputs/{folder}'\n",
    "combine_with_sketches = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8d310",
   "metadata": {},
   "source": [
    "The cell below define functions to rename files, in this case, the *Distorted* images. (in-house we called them freakshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfa884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_flickr(f_name):\n",
    "    file_id = f_name.strip('.jpg').split()[0]\n",
    "    return file_id\n",
    "\n",
    "\n",
    "def strip_freakshow(f_name):\n",
    "    file_id = f_name.strip('.jpg').split('_')[2]\n",
    "    return file_id\n",
    "\n",
    "\n",
    "def do_files_rename(path_to_data, dataset_from, folder_to):\n",
    "    for f_name in listdir(f'{path_to_data}/{dataset_from}'):\n",
    "        if dataset_from == 'flickr8k':\n",
    "            file_id = strip_flickr(f_name)\n",
    "        else: \n",
    "            file_id = strip_freakshow(f_name)\n",
    "        original = f'{path_to_data}/{dataset_from}/img/{f_name}'\n",
    "        target = f'{path_to_data}/{folder_to}/img/{file_id}.jpg'\n",
    "        shutil.copyfile(original, target)\n",
    "        \n",
    "def rename_and_create_flickr(main_path, path_to_old, path_to_new):\n",
    "    flick = dict()\n",
    "\n",
    "    flick_map, new_flick = dict(), dict()\n",
    "\n",
    "    with open(f'{path_to_old}/captions.txt', mode='r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip().split('.jpg,')\n",
    "            image_id, caption = line[0], line[1]\n",
    "            if image_id in flick:\n",
    "                flick[image_id].append(caption)\n",
    "            else:\n",
    "                flick[image_id] = list()\n",
    "                \n",
    "    for i, key in enumerate(flick):\n",
    "        flick_map[key] = i\n",
    "        new_flick[i] = flick[key]\n",
    "        with open(f'{path_to_new}/{i}.txt', mode='w') as f:\n",
    "            for item in flick[key]:\n",
    "                print(item, file=f)\n",
    "\n",
    "    do_files_rename(f'{main_path}/data', 'flickr8k', 'flickr')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c95dc",
   "metadata": {},
   "source": [
    "The cell below defines:\n",
    "- reading from dataset into a dictionary\n",
    "- creating word maps for encoding\n",
    "- creating `.hdf5` files, whcih are later used for training\n",
    "\n",
    "\n",
    "There is some leftover code (commented) for joining images and sketches into one embedding, yet the model was not adapted to deal with 6 channels instead of 3 -- future work.\n",
    "\n",
    "The logic for everything below is similar to that in `a-PyTorch-Tutorial-to-Image-Captioning`, yet we adapted it in such a way that there is no need for karpahty files or similar, just plain dictionaries of paths and captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff11e791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T19:03:43.182527Z",
     "start_time": "2022-01-17T19:03:43.170681Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # load doc into memory\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def read_create_dataset(directory, augment=False):\n",
    "    # two outputs -- path:caption and id:path dictionaries\n",
    "    paths_caption_dictn, id_path_dictn = dict(), dict()\n",
    "    path_to_data = f'{main_path}/data/{dataset}'\n",
    "    for f_name in tqdm(listdir(f'{path_to_data}/img')):\n",
    "        img_filepath = f'{path_to_images}/{f_name}'\n",
    "        jpgname = f_name.split('.')\n",
    "        if jpgname[1] == 'jpg':\n",
    "            image_id = int(jpgname[0])\n",
    "            if augment:\n",
    "                cap_filepath = f'{main_path}/data/augmented_captions/{image_id}.txt'\n",
    "            else:\n",
    "                cap_filepath = f'{path_to_data}/captions/{image_id}.txt'\n",
    "            id_path_dictn[image_id] = img_filepath\n",
    "            lines = load_doc(cap_filepath)\n",
    "            lines = lines.split('\\n')\n",
    "            paths_caption_dictn[img_filepath] = list()\n",
    "            for caption in lines[:10]:\n",
    "                caption = word_tokenize(caption)\n",
    "                caption =[token.lower() for token in caption if token != ',']\n",
    "                paths_caption_dictn[img_filepath].append(caption)\n",
    "        else: \n",
    "            print(f'Encountered random file: {f_name}')\n",
    "    return paths_caption_dictn, id_path_dictn\n",
    "\n",
    "\n",
    "\n",
    "def mixing_read_create_dataset(main_path):\n",
    "     # two outputs -- path:caption and id:path dictionaries\n",
    "    paths_caption_dictn, id_path_dictn = dict(), dict()\n",
    "    path_to_data = f'{main_path}/data/{dataset}'\n",
    "    path_to_flickr = f'{main_path}/data/flickr'\n",
    "    for f_name in tqdm(listdir(f'{path_to_data}/img')):\n",
    "        img_filepath = f'{path_to_data}/img/{f_name}'\n",
    "        jpgname = f_name.split('.')\n",
    "        if jpgname[1] != 'jpg':\n",
    "            print(f'Encountered random file: {f_name}')\n",
    "        else:\n",
    "            image_id = int(jpgname[0])\n",
    "            if mix_data and 7000 <= image_id < 8090:\n",
    "                img_filepath = f'{path_to_flickr}/img/{image_id}.jpg'\n",
    "                cap_filepath = f'{path_to_flickr}/captions/{image_id}.txt'\n",
    "            else:\n",
    "                cap_filepath = f'{path_to_data}/captions/{image_id}.txt'\n",
    "            id_path_dictn[image_id] = img_filepath\n",
    "            #load captions\n",
    "            lines = load_doc(cap_filepath)\n",
    "            lines = lines.split('\\n')\n",
    "            # add path + list to dictn\n",
    "            paths_caption_dictn[img_filepath] = list()\n",
    "            #take only 10 descriptions\n",
    "            for caption in lines[:10]:\n",
    "                if len(caption) > 3:\n",
    "                    # add a tokenized caption to path_captions dict\n",
    "                    caption = word_tokenize(caption)\n",
    "                    #caption = nltk.Text(caption)\n",
    "                    caption =[token.lower() for token in caption if token != ',']\n",
    "                    paths_caption_dictn[img_filepath].append(caption)\n",
    "\n",
    "    return paths_caption_dictn, id_path_dictn\n",
    "\n",
    "\n",
    "\n",
    "def partial_augment_read_create_dataset():\n",
    "    paths_caption_dictn, id_path_dictn = dict(), dict()\n",
    "    path_to_data = f'{main_path}/data/{dataset}'\n",
    "    for f_name in tqdm(listdir(f'{path_to_data}/img')):\n",
    "        img_filepath = f'{path_to_images}/{f_name}'\n",
    "        jpgname = f_name.split('.')\n",
    "        if jpgname[1] == 'jpg':\n",
    "            image_id = int(jpgname[0])\n",
    "            \n",
    "            alt_cap_filepath = f'{main_path}/data/augmented_captions/{image_id}.txt'\n",
    "            cap_filepath = f'{path_to_data}/captions/{image_id}.txt'\n",
    "            \n",
    "            id_path_dictn[image_id] = img_filepath\n",
    "            \n",
    "            lines = load_doc(cap_filepath)\n",
    "            lines = lines.split('\\n')\n",
    "            \n",
    "            alt_lines = load_doc(alt_cap_filepath)\n",
    "            alt_lines = alt_lines.split('\\n')\n",
    "            \n",
    "            paths_caption_dictn[img_filepath] = list()\n",
    "            \n",
    "            for i, caption in enumerate(lines[:10]):\n",
    "                if 3 < i < 7:\n",
    "                    caption = alt_lines[i]\n",
    "                caption = word_tokenize(caption)\n",
    "                caption =[token.lower() for token in caption if token != ',']\n",
    "                paths_caption_dictn[img_filepath].append(caption)\n",
    "                \n",
    "        else: \n",
    "            print(f'Encountered random file: {f_name}')\n",
    "    return paths_caption_dictn, id_path_dictn\n",
    "\n",
    "\n",
    "\n",
    "def dictn_reverser(dictn):\n",
    "    val_key = {v:k for k, v in dictn.items()}\n",
    "    return val_key\n",
    "\n",
    "def get_dict_from_file(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        dictn = json.load(f)\n",
    "    return dictn\n",
    "\n",
    "def create_word_map(data, max_cap_lenght):\n",
    "    word_freq, captions = dict(), list()\n",
    "    for path in data:\n",
    "        # read throuh every tokenized and lowrcased caption as a list of tokens\n",
    "        for captn in data[path]:\n",
    "            if len(captn) <= max_cap_lenght:\n",
    "                pass\n",
    "            else:\n",
    "                captn = captn[:max_cap_lenght]\n",
    "            for w in captn:\n",
    "                captions.append(w)\n",
    "                if w in word_freq:\n",
    "                    word_freq[w] += 1\n",
    "                else:\n",
    "                    word_freq[w] = 1\n",
    "    \n",
    "    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "    word_map = {k: v + 1 for v, k in enumerate(words)}\n",
    "    word_map['<unk>'] = len(word_map) + 1\n",
    "    word_map['<start>'] = len(word_map) + 1\n",
    "    word_map['<end>'] = len(word_map) + 1\n",
    "    word_map['<pad>'] = 0\n",
    "    \n",
    "    \n",
    "    return word_map, captions\n",
    "\n",
    "\n",
    "def create_h5py(directory, path_data, word_map, cap_per_img, max_cap_lenght, sketches_dictn, combine_with_sketches):\n",
    "    for subset in path_data:\n",
    "        with h5py.File(os.path.join(directory + '/'+ subset+'_'+base_filename+'.hdf5'), 'w') as h:\n",
    "            h.attrs['captions_per_image'] = cap_per_img\n",
    "            if combine_with_sketches:\n",
    "                images = h.create_dataset('images', (len(path_data[subset]), 6, 256, 256), dtype='uint8')\n",
    "            else:\n",
    "                images = h.create_dataset('images', (len(path_data[subset]), 3, 256, 256), dtype='uint8')\n",
    "    \n",
    "            enc_captions = []\n",
    "            caplens = []\n",
    "            for i, path in enumerate(tqdm(path_data[subset])):\n",
    "            # read sketch from folder\n",
    "                img_id = int(path.split('/')[-1].strip('.jpg'))\n",
    "                \n",
    "                imcaps = path_data[subset][path]\n",
    "                \n",
    "                \n",
    "                if len(imcaps) < cap_per_img:\n",
    "                    captions = imcaps + [choice(imcaps) for _ in range(cap_per_img - len(imcaps))]\n",
    "                elif len(imcaps) == cap_per_img:\n",
    "                    captions = imcaps\n",
    "                else:\n",
    "                    captions = sample(imcaps, k=cap_per_img)\n",
    "                \n",
    "                assert len(captions) == cap_per_img\n",
    "                \n",
    "\n",
    "                # Read images\n",
    "                img = Image.open(path)\n",
    "                img = img.resize((256, 256))\n",
    "                img = np.transpose(img, (2, 0, 1))\n",
    "                \n",
    "                # Below is our attempt of joining the sketches and images.\n",
    "                # The code is fully functional yet the model is still not adapted\n",
    "                \n",
    "#                 if combine_with_sketches:\n",
    "#                     sk_path = sketches_dictn[img_id]\n",
    "                    \n",
    "#                     sk = Image.open(sk_path)\n",
    "#                     sk = sk.resize((256, 256))\n",
    "#                     sk = np.transpose(sk, (2, 0, 1))\n",
    "                    \n",
    "#                     img = torch.from_numpy(img)\n",
    "#                     img = torch.unsqueeze(img, 0)\n",
    "                    \n",
    "#                     sk = torch.from_numpy(sk)\n",
    "#                     sk = torch.unsqueeze(sk, 0)\n",
    "                    \n",
    "#                     img = torch.cat((img, sk), 1)\n",
    "#                     img = torch.squeeze(img, 0)\n",
    "#                     img = img.cpu().detach().numpy()\n",
    "#                     img = np.transpose(img, (0, 1, 2))\n",
    "                    \n",
    "\n",
    "                assert np.max(img) <= 255\n",
    "\n",
    "                # Save image to HDF5 file\n",
    "                images[i] = img\n",
    "\n",
    "                # encoding the captions\n",
    "                for j, c in enumerate(captions):\n",
    "                    enc_c = [word_map['<start>']]\n",
    "                    for word in c:\n",
    "                        if word == '.':\n",
    "                            enc_c.append(word_map['<end>'])\n",
    "                        else:\n",
    "                            enc_c.append(word_map.get(word, word_map['<unk>']))\n",
    "                    \n",
    "                    additional = [word_map['<pad>'] for i in range((max_cap_lenght - len(c)))]\n",
    "                    enc_c.extend(additional)\n",
    "                    \n",
    "\n",
    "                    # Find caption lengths\n",
    "                    # because of start/end tokens, it is +2\n",
    "                    c_len = len(enc_c) + 2\n",
    "\n",
    "                    enc_captions.append(enc_c)\n",
    "                    caplens.append(len(c))\n",
    "            \n",
    "            # Sanity check\n",
    "            assert images.shape[0] * cap_per_img == len(enc_captions) == len(caplens)\n",
    "            \n",
    "\n",
    "            # Save encoded captions and their lengths to JSON files\n",
    "            with open(os.path.join(out_path, subset + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(enc_captions, j)\n",
    "            with open(os.path.join(out_path, subset + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
    "                json.dump(caplens, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2755c4",
   "metadata": {},
   "source": [
    "If you are plannin to use either `flickr8k` or *Distorted* data, use the cells below to rename and create files in desired format for subsequent code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_files_rename(f'{main_path}/data', 'freakshow_10k', 'freakshow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe416775",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mix_data: \n",
    "    path_to_old = '/Users/evelsve/repos/cap/data/flickr8k'\n",
    "    path_to_new = '/Users/evelsve/repos/cap/data/flickr'\n",
    "    \n",
    "    rename_and_create_flickr(main_path, path_to_old, path_to_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b8931",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T19:03:44.519023Z",
     "start_time": "2022-01-17T19:03:44.514831Z"
    }
   },
   "outputs": [],
   "source": [
    "if mix_data: \n",
    "    data, data_id = mixing_read_create_dataset(f'{main_path}')\n",
    "elif partial_anton_captions:\n",
    "    data, data_id = partial_augment_read_create_dataset(f'{main_path}')\n",
    "elif full_anton_captions:\n",
    "    data, data_id = read_create_dataset(f'{main_path}', augment=True)\n",
    "else:\n",
    "    data, data_id = read_create_dataset(f'{main_path}')\n",
    "\n",
    "path_data = dict()\n",
    "path_data['TRAIN'], path_data['TEST'], path_data['VAL']  = dict(), dict(), dict()\n",
    "\n",
    "for path, captions in data.items():\n",
    "    # take path, get img name, remove jpg, convert to int\n",
    "    img_id = int(path.split('/')[-1].strip('.jpg'))\n",
    "    if img_id < 8000:\n",
    "        path_data['TRAIN'][path] = captions\n",
    "    elif 8000 <= img_id < 9000:\n",
    "        path_data['VAL'][path] = captions\n",
    "    elif 9000 <= img_id < 10000:\n",
    "        path_data['TEST'][path] = captions\n",
    "        \n",
    "print('Step 1: Read data done.')\n",
    "    \n",
    "word_map, all_captions = create_word_map(data, max_cap_lenght)\n",
    "\n",
    "with open(os.path.join(out_path,'wordmap_'+base_filename+'.json'), 'w') as j:\n",
    "        json.dump(word_map, j)\n",
    "        \n",
    "        \n",
    "print('Step 2: Wordmap creation done.')\n",
    "        \n",
    "sketches_dictn = dict()\n",
    "\n",
    "if combine_with_sketches:\n",
    "    sketches_path = \"/Users/evelsve/repos/cap/data/sketches\"\n",
    "    for f_name in listdir(sketches_path):\n",
    "        img_id = int(f_name.split('/')[-1].strip('.jpg'))\n",
    "        path = sketches_path + '/' + f_name\n",
    "        sketches_dictn[img_id] = path\n",
    "        print('Step 1: Sketches path read done.')\n",
    "        print(f'INFO: Lenght of sketches: {len(sketches_dictn)}')\n",
    "        \n",
    "        \n",
    "print(f\"INFO: In train: {len(path_data['TRAIN'])}\\n In test: {len(path_data['TEST'])} \\n In val: {len(path_data['VAL'])}\")\n",
    "\n",
    "print(f'INFO: Lenght of the vocabulary: {len(word_map)}')\n",
    "\n",
    "create_h5py(main_path, path_data, word_map, cap_per_img, max_cap_lenght, sketches_dictn, combine_with_sketches)\n",
    "\n",
    "print('Step 3: Files for training, testing and validation created. END')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0dc2ad",
   "metadata": {},
   "source": [
    "If one wants to analyze the output or to make sure everything went smoothely, the code below can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b5d11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T19:09:21.295538Z",
     "start_time": "2022-01-17T19:09:21.213424Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "# filename = \"data/outputs/specific_output_folder/file_to_check.hdf5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # Get the data\n",
    "    data = list(f[a_group_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b8b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T19:10:02.073160Z",
     "start_time": "2022-01-17T19:10:02.068675Z"
    }
   },
   "outputs": [],
   "source": [
    "data[200][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8be0fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-17T12:35:43.614054Z",
     "start_time": "2022-01-17T12:35:43.587491Z"
    }
   },
   "outputs": [],
   "source": [
    "data_to_check = get_dict_from_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kiki",
   "language": "python",
   "name": "kiki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
